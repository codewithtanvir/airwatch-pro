groups:
  - name: airwatch-alerts
    rules:
      # High API Response Time
      - alert: HighAPIResponseTime
        expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="airwatch-backend"}[5m])) by (le)) > 2
        for: 5m
        labels:
          severity: warning
          service: backend
        annotations:
          summary: "High API response time detected"
          description: "95th percentile response time is {{ $value }}s for the past 5 minutes"

      # High Error Rate
      - alert: HighErrorRate
        expr: sum(rate(http_requests_total{job="airwatch-backend",status=~"5.."}[5m])) / sum(rate(http_requests_total{job="airwatch-backend"}[5m])) > 0.1
        for: 2m
        labels:
          severity: critical
          service: backend
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for the past 2 minutes"

      # Service Down
      - alert: ServiceDown
        expr: up{job=~"airwatch-.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} has been down for more than 1 minute"

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: (container_memory_usage_bytes{pod=~"airwatch-.*"} / container_spec_memory_limit_bytes) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"

      # High CPU Usage
      - alert: HighCPUUsage
        expr: (rate(container_cpu_usage_seconds_total{pod=~"airwatch-.*"}[5m]) / container_spec_cpu_quota * container_spec_cpu_period) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "Pod {{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }}"

      # Database Connection Issues
      - alert: DatabaseConnectionHigh
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "High database connections"
          description: "PostgreSQL has {{ $value }} active connections"

      # Redis Memory Usage
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "High Redis memory usage"
          description: "Redis memory usage is {{ $value | humanizePercentage }}"

      # Disk Space Low
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes) < 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Disk space is running low"
          description: "Filesystem {{ $labels.mountpoint }} has only {{ $value | humanizePercentage }} space left"

      # ML Model Performance Degradation
      - alert: MLModelPerformanceDegraded
        expr: ml_model_accuracy < 0.8
        for: 10m
        labels:
          severity: warning
          service: ml
        annotations:
          summary: "ML model performance degraded"
          description: "ML model accuracy has dropped to {{ $value | humanizePercentage }}"

      # NASA API Rate Limit
      - alert: NASAAPIRateLimit
        expr: nasa_api_rate_limit_remaining < 10
        for: 1m
        labels:
          severity: warning
          service: nasa-api
        annotations:
          summary: "NASA API rate limit approaching"
          description: "Only {{ $value }} NASA API requests remaining"

      # Alert Distribution Failure
      - alert: AlertDistributionFailure
        expr: sum(rate(alert_distribution_failures_total[5m])) > 0.1
        for: 2m
        labels:
          severity: critical
          service: alerts
        annotations:
          summary: "Alert distribution failures detected"
          description: "Alert distribution failure rate is {{ $value }} per second"

      # Kubernetes Pod Restart
      - alert: PodRestarting
        expr: increase(kube_pod_container_status_restarts_total{pod=~"airwatch-.*"}[1h]) > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Pod is restarting"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"

      # Node Not Ready
      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes node not ready"
          description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"

      # Persistent Volume Usage High
      - alert: PersistentVolumeUsageHigh
        expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Persistent volume usage high"
          description: "PV {{ $labels.persistentvolumeclaim }} usage is {{ $value | humanizePercentage }}"

      # Application Health Check Failing
      - alert: HealthCheckFailing
        expr: probe_success{job="airwatch-health-check"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Application health check failing"
          description: "Health check for {{ $labels.instance }} has been failing for 2 minutes"

      # SSL Certificate Expiry
      - alert: SSLCertificateExpiry
        expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 7
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{ $labels.instance }} expires in {{ $value | humanizeDuration }}"

  - name: nasa-tempo-alerts
    rules:
      # TEMPO Data Freshness
      - alert: TEMPODataStale
        expr: time() - tempo_last_data_timestamp > 3600
        for: 10m
        labels:
          severity: warning
          service: tempo
        annotations:
          summary: "TEMPO satellite data is stale"
          description: "No fresh TEMPO data received for {{ $value | humanizeDuration }}"

      # TEMPO Data Quality
      - alert: TEMPODataQualityLow
        expr: tempo_data_quality_score < 0.7
        for: 15m
        labels:
          severity: warning
          service: tempo
        annotations:
          summary: "TEMPO data quality is low"
          description: "TEMPO data quality score is {{ $value }}"

      # Air Quality Alert Threshold
      - alert: AirQualityUnhealthy
        expr: aqi_current > 150
        for: 5m
        labels:
          severity: critical
          service: air-quality
        annotations:
          summary: "Air quality is unhealthy"
          description: "Current AQI is {{ $value }} in {{ $labels.location }}"

      # Public Health Risk
      - alert: PublicHealthRiskHigh
        expr: vulnerable_population_exposure_count > 1000
        for: 10m
        labels:
          severity: critical
          service: public-health
        annotations:
          summary: "High public health risk detected"
          description: "{{ $value }} vulnerable individuals exposed to unhealthy air quality"

  - name: infrastructure-alerts
    rules:
      # Load Balancer Health
      - alert: LoadBalancerUnhealthy
        expr: aws_elb_healthy_host_count < 1
        for: 2m
        labels:
          severity: critical
          service: load-balancer
        annotations:
          summary: "Load balancer has no healthy targets"
          description: "ELB {{ $labels.load_balancer_name }} has no healthy targets"

      # Auto Scaling Activity
      - alert: AutoScalingEvent
        expr: increase(aws_autoscaling_group_desired_capacity[5m]) != 0
        for: 0m
        labels:
          severity: info
          service: autoscaling
        annotations:
          summary: "Auto scaling event occurred"
          description: "Auto scaling group {{ $labels.auto_scaling_group_name }} capacity changed"

      # RDS Connection Issues
      - alert: RDSConnectionIssues
        expr: aws_rds_database_connections > 80
        for: 5m
        labels:
          severity: warning
          service: rds
        annotations:
          summary: "High RDS connections"
          description: "RDS instance {{ $labels.db_instance_identifier }} has {{ $value }} connections"